{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from skimage.transform import resize\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "    \"\"\"\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    \n",
    "    The jaccard distance loss is usefull for unbalanced datasets. This has been\n",
    "    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
    "    gradient.\n",
    "    \n",
    "    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    \n",
    "    @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose source and receiver types here \n",
    "### Prefix used for input and output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls ./horizontal_data/*.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_string = 'horizontal'\n",
    "receiver_string = 'das'  #'combined', 'geophone' 'das' \n",
    "foldstring = 0 #0, 1, or 2\n",
    "values4random_state=[1234, 69, 753]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if source_string == 'horizontal':\n",
    "    data = np.load(r'./horizontal_data/h_'+receiver_string+'_data.npy') \n",
    "    labels = np.load(r'./horizontal_data/h_fault_labels.npy')\n",
    "else:\n",
    "    data = np.load(r'./vertical_data/v_'+receiver_string+'_data.npy')\n",
    "    labels = np.load(r'./vertical_data/v_fault_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx = 256\n",
    "ny = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resized_data = np.zeros((len(data), nx, ny, 1))\n",
    "resized_labels = np.zeros((len(data), nx, ny, 2))\n",
    "for i in range(len(resized_data)):\n",
    "    resized_data[i,:,:,0] = resize(data[i,:,:,0], (nx, ny))\n",
    "    resized_labels[i,:,:,1] = resize(labels[i,:,:,0], (nx, ny)) # flip on purpose?\n",
    "    resized_labels[i,:,:,0] = resize(labels[i,:,:,1], (nx, ny))\n",
    "resized_labels[resized_labels <= 0.5] = 0\n",
    "resized_labels[resized_labels > 0.5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardization across entire dataset\n",
    "data_mean = np.mean(resized_data)\n",
    "data_std = np.std(resized_data)\n",
    "data_scaled = (resized_data-data_mean)/data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = data_scaled\n",
    "y = resized_labels[:,:,:,1]\n",
    "y = np.expand_dims(y, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How did Michael do the Cross Validation if he fixed the train-test split?\n",
    "I'm giving each fold a different ``random_state``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split train and valid\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(x, y, test_size=0.20, shuffle=True, random_state=values4random_state[foldstring])\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.25, shuffle=True, random_state=values4random_state[foldstring])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Data augmentation on just the training data\n",
    "## Image Augmentation\n",
    "# Vertical Image\n",
    "#Vx = [np.flip(x, axis=1) for x in x_train]\n",
    "#Vy = [np.flip(x, axis=1) for x in y_train]\n",
    "\n",
    "# Horizontal Image\n",
    "#Hx = [np.flip(x, axis=2) for x in x_train]\n",
    "#Hy = [np.flip(x, axis=2) for x in y_train]\n",
    "\n",
    "# Horizontal Vertical Image\n",
    "#HVx = [np.flip(x, axis=2) for x in Vx]\n",
    "#HVy = [np.flip(x, axis=2) for x in Vy]\n",
    "\n",
    "# Appending the augmented image and mask to the main dataset.\n",
    "#x_train = np.append(x_train, Vx, axis=0)\n",
    "#y_train = np.append(y_train, Vy, axis=0)\n",
    "\n",
    "#x_train = np.append(x_train, Hx, axis=0)\n",
    "#y_train = np.append(y_train, Hy, axis=0)\n",
    "\n",
    "#x_train = np.append(x_train, HVx, axis=0)\n",
    "#y_train = np.append(y_train, HVy, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, n_filters, kernel_size=3, l2_lambda=5e-5, batch_norm=True):\n",
    "    \"\"\"\n",
    "    A single convolution block for Unet.\n",
    "    Convolution -> Batch Normalization -> Activation -> Repeat\n",
    "    \"\"\"\n",
    "    # first layer\n",
    "    conv_0 = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "               padding=\"same\", kernel_regularizer=regularizers.l2(l2_lambda))(input_tensor)\n",
    "    if batch_norm:\n",
    "        conv_0 = BatchNormalization()(conv_0)\n",
    "    conv_0 = Activation(\"relu\")(conv_0)\n",
    "    # second layer\n",
    "    conv_1 = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer=\"he_normal\",\n",
    "               padding=\"same\", kernel_regularizer=regularizers.l2(l2_lambda))(conv_0)\n",
    "    if batch_norm:\n",
    "        conv_1 = BatchNormalization()(conv_1)\n",
    "    conv_1 = Activation(\"relu\")(conv_1)\n",
    "    return conv_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_unet(input_image, n_filters=16, dropout=0.5, batch_norm=True):\n",
    "    \"\"\"\n",
    "    Create U-net similar to that described by Ronneberger et al. 2015, with some changes:\n",
    "    Output size will equal input size for all convolutioin operations\n",
    "    Dropout is applied after every max pooling or upconvolution operation.\n",
    "    L2 regularization is applied after every convolution\n",
    "    \"\"\"\n",
    "    # downsampling layers\n",
    "    c_d_0 = conv2d_block(input_image, n_filters=n_filters, kernel_size=3, batch_norm=batch_norm)\n",
    "    p_d_0 = MaxPooling2D((2,2))(c_d_0)\n",
    "    d_d_0 = Dropout(dropout*0.5)(p_d_0)\n",
    "\n",
    "    c_d_1 = conv2d_block(d_d_0, n_filters=n_filters*2, kernel_size=3, batch_norm=batch_norm)\n",
    "    p_d_1 = MaxPooling2D((2,2))(c_d_1)\n",
    "    d_d_1 = Dropout(dropout)(p_d_1)\n",
    "\n",
    "    c_d_2 = conv2d_block(d_d_1, n_filters=n_filters*4, kernel_size=3, batch_norm=batch_norm)\n",
    "    p_d_2 = MaxPooling2D((2,2))(c_d_2)\n",
    "    d_d_2 = Dropout(dropout)(p_d_2)\n",
    "\n",
    "    c_d_3 = conv2d_block(d_d_2, n_filters=n_filters*8, kernel_size=3, batch_norm=batch_norm)\n",
    "    p_d_3= MaxPooling2D(pool_size=(2,2))(c_d_3)\n",
    "    d_d_3 = Dropout(dropout)(p_d_3)\n",
    "    \n",
    "    c_d_4 = conv2d_block(d_d_3, n_filters=n_filters*16, kernel_size=3, batch_norm=batch_norm)\n",
    "    \n",
    "    # upsampling layers\n",
    "    u_u_3 = Conv2DTranspose(n_filters*8, (3,3), strides=(2,2), padding='same')(c_d_4)\n",
    "    # skip layer 3\n",
    "    cc_u_3 = concatenate([u_u_3, c_d_3])\n",
    "    d_u_3 = Dropout(dropout)(cc_u_3)\n",
    "    c_u_3 = conv2d_block(d_u_3, n_filters=n_filters*8, kernel_size=3, batch_norm=batch_norm)\n",
    "\n",
    "    u_u_2 = Conv2DTranspose(n_filters*4, (3,3), strides=(2,2), padding='same')(c_u_3)\n",
    "    cc_u_2 = concatenate([u_u_2, c_d_2])\n",
    "    # skip layer 2\n",
    "    d_u_2 = Dropout(dropout)(cc_u_2)\n",
    "    c_u_2 = conv2d_block(d_u_2, n_filters=n_filters*4, kernel_size=3, batch_norm=batch_norm)\n",
    "\n",
    "    u_u_1 = Conv2DTranspose(n_filters*2, (3,3), strides=(2,2), padding='same')(c_u_2)\n",
    "    # skip layer 1\n",
    "    cc_u_1 = concatenate([u_u_1, c_d_1])\n",
    "    d_u_1 = Dropout(dropout)(cc_u_1)\n",
    "    c_u_1 = conv2d_block(d_u_1, n_filters=n_filters*2, kernel_size=3, batch_norm=batch_norm)\n",
    "\n",
    "    u_u_0 = Conv2DTranspose(n_filters, (3,3), strides=(2,2), padding='same')(c_u_1)\n",
    "    # skip layer 0\n",
    "    cc_u_0 = concatenate([u_u_0, c_d_0], axis=3)\n",
    "    d_u_0 = Dropout(dropout)(cc_u_0)\n",
    "    c_u_0 = conv2d_block(d_u_0, n_filters=n_filters, kernel_size=3, batch_norm=batch_norm)\n",
    "    \n",
    "    output_segmentation = Conv2D(1, (1, 1), activation='sigmoid')(c_u_0)\n",
    "    model = Model(inputs=[input_img], outputs=[output_segmentation])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_img = Input((nx, ny, 1), name='img')\n",
    "\n",
    "model = create_unet(input_img, n_filters=16, dropout=0.025, batch_norm=True)\n",
    "model.compile(optimizer=Adam(), loss=dice_coef_loss, metrics=[\"accuracy\", f1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!ls -ltr */models .\\models\\\n",
    "!ls -ltr models/*.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'models/'+source_string[0]+'-model-'+receiver_string+'-dice-fold-'+str(foldstring)+'.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=2, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint('models/'+source_string[0]+'-model-'+receiver_string+'-dice-fold-'+str(foldstring)+'.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "# do stuff\n",
    "elapsed = time.time() - t\n",
    "print('elapsed time',elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. ```results = ``` cell to perform training\n",
    "\n",
    "Horizontal DAS took 32 minutes (models/h-model-das-test.h5) : But only has 20 epochs, Michael recommends 50-100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "results = model.fit(x_train, y_train, batch_size=1, epochs=20, callbacks=callbacks,\n",
    "                   validation_data=(x_val, y_val))\n",
    "elapsed = time.time() - t\n",
    "print('elapsed time',elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"log_loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#?model.evaluate\n",
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `model.evaluate()` Returns...\n",
    "    Scalar test loss (if the model has a single output and no metrics)\n",
    "    or list of scalars (if the model has multiple outputs\n",
    "    and/or metrics). The attribute `model.metrics_names` will give you\n",
    "    the display labels for the scalar outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate on validation data\n",
    "model.evaluate(x_val, y_val, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict on train, val and test\n",
    "preds_train = model.predict(x_train, verbose=1)\n",
    "preds_val = model.predict(x_val, verbose=1)\n",
    "preds_test = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Threshold predictions\n",
    "preds_train_t = (preds_train > 0.5).astype(np.uint8)\n",
    "preds_val_t = (preds_val > 0.5).astype(np.uint8)\n",
    "preds_test_t = (preds_test > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_sample(X, y, preds, binary_preds, ix=None, filename=None):\n",
    "    if ix is None:\n",
    "        ix = random.randint(0, len(X))\n",
    "        print('random slice is:',ix)\n",
    "\n",
    "    has_mask = y[ix].max() > 0\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(20, 10))\n",
    "    im0 = ax[0,0].imshow(X[ix, ..., 0], cmap='gray')\n",
    "    #if has_mask:\n",
    "        #ax[0].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
    "    fig.colorbar(im0, ax=ax[0,0], fraction=0.046, pad=0.04)\n",
    "    ax[0,0].set_title('Seismic')\n",
    "\n",
    "    im1 = ax[0,1].imshow(y[ix].squeeze())\n",
    "    ax[0,1].set_title('Fault')\n",
    "    fig.colorbar(im1, ax=ax[0,1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    im2 = ax[1,0].imshow(preds[ix].squeeze(), vmin=0, vmax=1)\n",
    "    #if has_mask:\n",
    "        #ax[2].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
    "    fig.colorbar(im2, ax=ax[1,0], fraction=0.046, pad=0.04)\n",
    "    ax[1,0].set_title('Fault Predicted')\n",
    "    \n",
    "    im3 = ax[1,1].imshow(binary_preds[ix].squeeze(), vmin=0, vmax=1)\n",
    "    #if has_mask:\n",
    "        #ax[3].contour(y[ix].squeeze(), colors='k', levels=[0.5])\n",
    "    fig.colorbar(im3, ax=ax[1,1], fraction=0.046, pad=0.04)\n",
    "    ax[1,1].set_title('Fault Predicted (Binary)')\n",
    "    fig.tight_layout();\n",
    "    if filename!=None:\n",
    "        plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if training data looks all right\n",
    "plot_sample(x_train, y_train, preds_train, preds_train_t, ix=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if valid data looks all right\n",
    "plot_sample(x_val, y_val, preds_val, preds_val_t, ix=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if test data looks all right\n",
    "plot_sample(x_test, y_test, preds_test, preds_test_t, ix=1, filename=source_string+receiver_string+'predictions_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_total = np.concatenate((x_train, x_val, x_test), axis=0)\n",
    "y_total = np.concatenate((y_train, y_val, y_test), axis=0)\n",
    "pred_total = np.concatenate((preds_train_t, preds_val_t, preds_test_t), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_total.ravel(), pred_total.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_roc_metrics(y_real, y_predict):\n",
    "\n",
    "    c_matrix = confusion_matrix(y_real.ravel(), y_predict.ravel())\n",
    "    f1 = f1_score(y_real.ravel(), y_predict.ravel())\n",
    "    recall = recall_score(y_real.ravel(), y_predict.ravel())\n",
    "    precision = precision_score(y_real.ravel(), y_predict.ravel())\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(c_matrix)\n",
    "    print(\"F1 score: {:.4f}\".format(f1))\n",
    "    print(\"Recall score: {:.4f}\".format(recall))\n",
    "    print(\"Precision score: {:.4f}\".format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_roc_metrics(y_test, preds_test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "''?confusion_matrix\n",
    "Thus in binary classification, the count of \n",
    "\n",
    "true negatives is :math:`C_{0,0}`, \n",
    "false negatives is :math:`C_{1,0}`, \n",
    "true positives is :math:`C_{1,1}` and \n",
    "false positives is :math:`C_{0,1}`.''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
